This chapter will discuss the evaluation process of the application including testing, evaluation experiments, and the results gathered.

\section{Testing}
Application testing was performed during, and, after the implementation process of this project. Testing is vitally important in software development to ensure code is bug free, and, ready for deployment.

\subsection{User Testing}
While the application was being developed, new features were consistently tested by having the application run locally, then experimenting with the new features created. Potential users were recruited throughout to test the application and provide feedback. This technique provided invaluable feedback, and, testing of the application, helping to create a more robust, and enjoyable user experience.

\subsection{Jest Testing}
Jest is a testing framework that allows the easy creation of tests for JavaScript code. Jest was used to create unit, and snapshot tests for this project. Unit testing is where individual `units' of an application are tested independently, this method was used to test functions, and, expected behaviour of the application. 

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \begin{lstlisting}[language=jsJsx]
        test("correct conversion", () => {
          expect(polarToCart(0, 0, 5, 90)).toStrictEqual({ x: 5, y: 0 });
        });
        test("navigation to home", () => {
            fireEvent.press(screen.getByTestId("pressableIcon"));
            expect(navigation.navigate).toBeCalledWith("HomeScreen");
        });
        \end{lstlisting}
    \end{subfigure}
\caption[Example of a unit test]{Example of a unit test on a function converting polar coordinates, to Cartesian, and, navigation.}
\label{fig:jestUnit}
\end{figure}
\FloatBarrier

Here we can see a test is created that expects a certain return value from the \texttt{polarToCart} function, this will either pass or fail depending on this value. Unit tests were also used to test aspects such as navigation, as seen above we are expecting the navigate function to be called with a parameter of "HomeScreen" when the \texttt{pressableIcon} has been pressed. These tests were used throughout the process of testing the components, and screens of the application. Jest also provides support for snapshot testing; this is a method used to test that the user interface has not unexpectedly changed. 

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \begin{lstlisting}[language=jsJsx]
        test("Check GroupScreen loading correctly", async () => {
          const domTree = renderer
            .create(<GroupScreen/>)
            .toJSON();
          expect(domTree).toMatchSnapshot();
        });
        \end{lstlisting}
    \end{subfigure}
\caption{Example of a snapshot test on the GroupScreen}
\label{fig:jestSnap}
\end{figure}
\FloatBarrier

Here a snapshot test is created to check the user interface is loading correctly for the \texttt{GroupScreen}. The first time a snapshot test is run Jest creates, and stores a JSON snapshot of the user interface. When run again, this snapshot is then compared against the current user interface for differences, failing if the two snapshots differ. All tests for the application were executed through an \texttt{npm test} command, and, on commits to the main, and, test branches from a GitHub action \ref{ghActions}.

\subsection{Testing Challenges}\label{testChall}

The main challenge that presented itself during testing was application code relying on Firebase. As all screens contained application, and user interface code this meant these elements could not be separately tested as the user interface relied on this application code. Many solutions were investigated but unfortunately none were suitable within the time constraints of the project. To be able to fully test this application, a Firebase local emulator suite would have to be set up to run tests locally. To learn and set up this emulator suite along with writing these tests was not feasible but is a feature that would be great to integrate in future releases for greater test coverage. The alternative was to mock all the Firebase functions, and, services that were used to provide mocked inputs to the user interface, and application functions, unfortunately this was also infeasible within the time constraints. Due to this, full test coverage was not able to be achieved for all screens, however, components were unaffected so an average of 84.31\% of the lines of component code were covered by tests. The user interface of screens was also tested with snapshot, and unit testing establishing some test coverage.   


\section{Experiment}
To evaluate the application an experiment was planned to gather user feedback, and measure usability. The initial plan was to have participants download the application, then complete the experiment. However, it was discovered that if the user was using an iOS device, they could not simply access a link and download the application. Apple's TestFlight platform is designed to alleviate this struggle by allowing developers to upload builds and have users download the application from there, this however is part of Apple's developer program, a paid service, so was not suitable. It was decided at this stage that two similar experiments were to be run, one in person on a device with the application installed, and an online survey showcasing application screenshots and recordings. This approach was taken to ensure there was a sufficient number of participants in the evaluation due to the in-person experiment being run solely on one device, hampering the ability to gather responses. 

\subsection*{Ethics}
Both experiments were performed in line with the University of Glasgow's School of Computer Science ethics checklist \cite{ethics}. All participants consented, were allowed to ask any questions, and were provided with contact details. A brief, and debrief were provided to all participants stating the aims of the experiment, and, reminding the participant they are free to stop at any time.

\subsection*{Participants}
A wide variety of ages, genders, and technological backgrounds were consulted in this evaluation. The majority of the participants were part of the target demographic for the application, aged eighteen to thirty. 

\subsection{In Person}
The goals of the in-person evaluation were to measure the usability of application, and, gather user feedback on their experience. The participant was first presented with some introductory questions to gain some initial opinions on the problem this application aims to solve, usage of similar concepts, and, the concept of this application (see\ref{appendix:introQns}).
Evaluating usability comprised of the participant completing a set of tasks, after each task the participant was asked the single ease question \cite{seq}, \textit{How difficult did you find this task?}. This was answered through a five-point Likert scale ranging from very difficult, to very easy. The tasks were designed to incorporate all key areas of the application, creating accounts and groups, adding friends, and, customizing the user experience (see \ref{appendix:tasks}). This was further complemented by the use of the system usability scale (SUS) \cite{sus}, the industry standard for evaluating usability of products. The SUS comprises of ten questions answered through a five point Likert scale ranging from strongly disagree, to strongly agree (see \ref{appendix:susQns}). This was completed by the participant after all the tasks. Upon completion the participant was asked some concluding questions used to gather user feedback after usage of the application (see \ref{appendix:concludQns}). 
For an example survey response see \cite{evalRespWApp}.

\subsection{Online}
The online evaluation consisted of the same introductory, and, concluding questions but with a change to the usability evaluation. As the user would not have direct interaction with the application, the usability study performed during the in-person evaluation was not suitable. Instead, two screen recordings of an example user interacting with the application were created, evaluated through Likert scale questions regarding the video content (see \ref{appendix:vidQns}). The participant was then asked to answer some general questions regarding the application and were given several screenshots of the application for reference (see \ref{appendix:onlineGenQns}). Although it was not possible to use the system usability scale, questions were designed to be very similar to gather meaningful results. For an example survey response see \cite{evalRespNoApp}. 

\section{Results}
In total there were 32 participants in the experiment, 24 online, and, 8 in person. The combination of both an in person, and online evaluation allowed the gathering of a suitable number of responses which would not have been possible with a purely in person evaluation.

\subsection{Introductory \& Concluding Questions}
The responses to the introductory, and concluding questions have been combined and presented below. The results from the introductory questions \ref{fig:introAns}, confirm that people do have issues with planning activities due to being unaware of their friend's status (30/32, 94\%), the problem this application aims to solve. Participants show a clear interest in the application concept with 91\% (29/32) being likely to use an app to graphically see their friend's status. 

\begin{figure}[H]

\begin{tikzpicture}  
\begin{axis}  
[  
    ybar,
    width=10cm,
    height=6.3cm,
    enlarge x limits=0.4,     
    ylabel={Number of Responses},
    ymin=0,
    symbolic x coords={Question 1, Question 2, Question 3},  
    xtick=data,  
    nodes near coords,  
    nodes near coords align={vertical},  
    legend style={at={(1.05,0.5)},anchor=west, legend columns=-1},
    ]  
\addplot coordinates {(Question 1, 30) (Question 2, 18) (Question 3, 29)};
\addplot coordinates {(Question 1, 2) (Question 2, 14) (Question 3, 3)};  
\legend{Yes, No}  
  
\end{axis}  
\end{tikzpicture} 
\begin{enumerate}
    \item Do you have issues with planning activities due to being unaware of your friend's status?
    \item Do you use snapchat maps or life360 for example to see what your friends are up to?
    \item Would you be likely to use an app to graphically see your friend's status?
\end{enumerate}

\caption{Combined results of introductory questions}
\label{fig:introAns}
\end{figure}
\FloatBarrier

The responses to the concluding questions from both experiments show a clear trend of participant interest in the concept, and application itself with 84\% (27/32) interested in the concept, and 66\% (21/32) reporting they would potentially use the application. All participants responded either yes, or, maybe with zero participants responding no to either of the questions.

\begin{figure}[H]

\begin{tikzpicture}  
\begin{axis}  
[  
    ybar,
    width=8cm,
    height=6cm,
    enlarge x limits=1,       
    ylabel={Number of Responses},
    ymin=0,
    ymax=30,
    symbolic x coords={Question 1, Question 2},  
    xtick=data,  
    nodes near coords,  
    nodes near coords align={vertical},  
    legend style={at={(1.05,0.5)},anchor=west, legend columns=-1},
    ]  
\addplot coordinates {(Question 1, 27) (Question 2, 21)};
\addplot coordinates {(Question 1, 5) (Question 2, 11)}; 
\addplot coordinates {(Question 1, 0) (Question 2, 0)}; 
\legend{Yes, Maybe, No}  
  
\end{axis}  
\end{tikzpicture} 
\begin{enumerate}
    \item After using the app are you interested in the concept?
    \item Would you potentially use such an app?
\end{enumerate}
\caption{Combined results of concluding questions}
\label{fig:conclAns}
\end{figure}
\FloatBarrier

\subsection{System Usability Scale (SUS)}
The system usability scale was completed by 8 participants during the in-person evaluation, after the user had completed all of the tasks. Calculating the SUS score involves subtracting 1 from the responses to the positively worded questions, then, responses to the negatively worded questions are subtracted from 5. These values are then summed together and multiplied by 2.5 to give a SUS score in the range 0-100. The participant responses to each question, and, corresponding SUS score are shown in \ref{appendix:susQns}. The average SUS score derived from all responses is 84.38, obtaining the highest grade of A+ according to the grading scale proposed by James Lewis and Jeff Sauro \cite{susGrades}. These results are a reliable measure of system usability, portraying how participants were impressed by the usability of the application.

\subsection{Online Evaluation Questions}

The online evaluation was completed by 24 participants after viewing the videos, and screenshots of the application. All questions were answered through a five-point Likert scale ranging from 1 being a very positive response, to 5 being a very negative response. This applies to all questions except question 4 in the general question section where the scale is reversed. The first video introduced the application, this covered creating profiles, customizing profiles, and the adding of friends. The responses to this video were very positive with most participants responding that the application looked very easy to navigate, the design was very consistent, and there was a sufficient level of customisation for profiles, shown by average responses ranging from 1 to 1.5. The second video showcased the creation, customisation, and general usage of groups. Participants again responded positively stating that the processes shown looked very easy to perform, the design was very consistent, and the level of customisation for groups was sufficient, with average responses ranging from 1.21 to 1.42. The general question section was answered with respect to the videos watched, and, the screenshots presented. Again, participant responses were very positive with participants liking the consistent design, and, thinking that the application seemed easy to use, navigate, and learn. The average responses are shown below with a full table of responses at \ref{appendix:onlineQnResp}.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l||l|l|l||l|l|l|l|}
    \hline
    \multicolumn{10}{|c|}{\textbf{Average Response }} \\
    \hline
     \multicolumn{3}{|c||}{Video 1 } &
     \multicolumn{3}{|c||}{Video 2 } &
     \multicolumn{4}{|c|}{General } \\
    \hline
        \textbf{a} & \textbf{b} & \textbf{c} & \textbf{a} & \textbf{b} & \textbf{c} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\ \hline
    \hline
        1.50 & 1.29 & 1.50 & 1.42 & 1.21 & 1.33 & 1.75 & 1.42 & 1.25 & 4.58 \\ \hline
        \multicolumn{10}{|c|}{\textbf{Standard Deviation }} \\ \hline
        0.59 & 0.86 & 0.51 & 0.65 & 0.41 & 0.56 & 0.74 & 0.72 & 0.44 & 0.50 \\ \hline
    \end{tabular}
\end{table}

\textbf{Online Evaluation Video Questions}
\vskip\baselineskip
\begin{enumerate}[label=Video \arabic*:  ,leftmargin=*]
    \item Register, Login, Editing of profile, Friends, Screen layout 
    \begin{enumerate}
        \item Did you think the app looked easy to navigate?
        \item Did you think the design of the screens was consistent?
        \item Do you think the level of customisation for profiles is sufficient?
    \end{enumerate}
    \vskip\baselineskip
    \item Creation of a group, Group customisation, General groups operation
    \begin{enumerate}
        \item Did you think this process looked easy to learn and perform?
        \item Did you think the design of the screens was consistent?
        \item Do you think the level of customisation for groups was sufficient?
    \end{enumerate}

\end{enumerate}
\vskip\baselineskip

\textbf{Online Evaluation General Application Questions}
\vskip\baselineskip
\begin{enumerate}
    \item I really liked the design of the application.
    \item I thought this app seemed very easy to use and navigate.
    \item I would imagine that most people would learn to use this system very quickly.
    \item I thought there was too much inconsistency in this system.
\end{enumerate}
\vskip\baselineskip
\newpage

\begin{figure}[H]

\begin{tikzpicture}  
\begin{axis}  
[  
    ybar,
    width=10cm,
    height=6.5cm,
    enlarge x limits=0.4,     
    ylabel={Average Response},
    ymin={1},
    ymax={5},
    ytick={1,2,3,4,5},
    symbolic x coords={Question a, Question b, Question c},  
    xtick=data,  
    nodes near coords style ={
        black,
        font=\small ,
        xshift={15}
    } , nodes near coords
    ]  
\addplot+[
            error bars/.cd,
            y dir=both,
            y explicit,
            error bar style ={color=red}
             ] coordinates {
             (Question a, 1.50) +- (0, 0.59)
             (Question b, 1.29) +- (0, 0.86)
             (Question c, 1.50) +- (0, 0.51)
             };
  
\end{axis}  
\end{tikzpicture}
\caption{Results of video 1 questions}
\label{fig:vid1Ans}
\end{figure}
\FloatBarrier

\begin{figure}[H]

\begin{tikzpicture}  
\begin{axis}  
[  
    ybar,
    width=10cm,
    height=6.5cm,
    enlarge x limits=0.4,     
    ylabel={Average Response},
    ymin={1},
    ymax={5},
    ytick={1,2,3,4,5},
    symbolic x coords={Question a, Question b, Question c},  
    xtick=data,  
    nodes near coords style ={
        black,
        font=\small ,
        xshift={15}
    } , nodes near coords
    ]  
\addplot+[
            error bars/.cd,
            y dir=both,
            y explicit,
            error bar style ={color=red}
             ] coordinates {
             (Question a, 1.42) +- (0, 0.65)
             (Question b, 1.21) +- (0, 0.41)
             (Question c, 1.33) +- (0, 0.56)
             };
  
\end{axis}  
\end{tikzpicture}
\caption{Results of video 2 questions}
\label{fig:vid2Ans}
\end{figure}
\FloatBarrier

\begin{figure}[H]

\begin{tikzpicture}  
\begin{axis}  
[  
    ybar,
    width=10cm,
    height=6.5cm,
    enlarge x limits=0.25,     
    ylabel={Average Response},
    ymin={1},
    ymax={5.5},
    ytick={1,2,3,4,5},
    symbolic x coords={Question 1, Question 2, Question 3, Question 4},  
    xtick=data,  
    nodes near coords style ={
        black,
        font=\small ,
        xshift={15}
    } , nodes near coords
    ]  
\addplot+[
            error bars/.cd,
            y dir=both,
            y explicit,
            error bar style ={color=red}
             ] coordinates {
             (Question 1, 1.75) +- (0, 0.74)
             (Question 2, 1.42) +- (0, 0.72)
             (Question 3, 1.25) +- (0, 0.44)
             (Question 4, 4.58) +- (0, 0.50)
             };
  
\end{axis}  
\end{tikzpicture}
\caption{Results of general questions}
\label{fig:genQAns}
\end{figure}
\FloatBarrier


\subsection{Participant Feedback}

Throughout both evaluations there was multiple opportunities to give textual feedback regarding the application. These free text responses have been split into two categories, application feedback, and suggested improvements. Some examples from both categories are listed below.
\vskip\baselineskip
\textbf{Application Feedback:}
\begin{itemize}
    \item \textit{"The app was very clean and crisp, while also looking easy to navigate through and use for multiple age ranges."} - This feedback complements the clean design and navigation of the application stating the app is easy to use, these were key areas that were focused upon during this pproject.
    \item \textit{"The design was consistent and used formats that are recognisable to the common user which is good because it utilises formats that the user already knows how to use."} - This feedback praises the consistent application design, an important aspect of the application. The recognisable formats and design concepts that were used to enhance the user experience were also praised by the participant.
    \item \textit{"Great customisation options. Improves user experience"} - Adding customisation options was very important, this participant states that there are great options available which is important for driving user engagement and enhancing user experience as stated by the participant.
    \item \textit{"Just minor things about the app like loading states \& feedbacks. A lot of Nielsen's heuristics are respected!"} - This user mentions Nielsen's heuristics which were a key consideration during the design, and implementation phases of the project. This is great as this user appreciates the effort put into making a usable application.
    \item \textit{"I think this app would be very useful and believe that it would be popular with young people and I know that I would use it along with my friends"} - This feedback shows how participants were reacting positively to the application, and how this participant would want to use the application with their peers.
    \item \textit{"Colours worked well together and was a very easy to learn system"} - The colour scheme of the app is praised along with the usability of the application with the user reporting the application being very easy to use. 
\end{itemize}
\vskip\baselineskip
In general, participants were very pleased with the application with many liking the consistent design and ease of use. The use of recognised design patterns and attempts to increase usability were noted by participants, this was excellent feedback to receive.   
\vskip\baselineskip
\textbf{Suggested Improvements:}
\begin{itemize}
    \item \textit{"When registering, users should be logged in automatically; rather than putting in their details again"} - This could be achieved by storing the user's login details after registration, then auto-filling the log in form with this data.
    \item \textit{"Possible light/dark mode toggle"} - A dark mode setting could be added with the design shifting to a darker, or lighter colour scheme.
    \item \textit{"As an alternative to the clock, clicking show members could show a list of statuses. This would be useful in case of a large number in a group."} - When the group members are shown after pressing show members, the user's status, as well as hand colour could be displayed.
    \item \textit{"The ability to change the background colour from purple."} - This could be a feature created in the application settings where the user could select from multiple application themes.
    \item  \textit{"In general I think it's a really cool idea for an app, but I wonder if in practice people using it would remember to change their status regularly to update their friends on their whereabouts. Perhaps some notifications could work well here. "} - Different methods of updating status could be investigated, push notifications to remind a user to update their status, or even a user's GPS location could be used to automatically update a user's status.
\end{itemize}
\vskip\baselineskip
There were many improvements suggested by participants which are invaluable, this presents new ideas, and, feature that could be implemented in future iterations, improving the application. Some noticeable feedback was that users would like the ability to change theme colours and have light and dark mode toggles. This was a `won't have' requirement listed in \ref{non-functional}, so although this release did not have this feature due to being limited by time constraints, this was a feature that could be implemented in the future. Integration with push notifications was also mentioned by participants, this was another feature listed (see \ref{functional}) that unfortunately could not be implemented in this release, but hopefully in the future. Other suggestions such as automatic logins, and, listing of member statuses were new ideas that could now be considered for a future release.

\subsection{In Person Experiment Observations}

As the in-person evaluation was observed, some extra insight was gained into how users were interacting with the application, and completing tasks. Behaviour was noted that was previously not seen, or considered during the implementation, and, testing of the application.
\vskip\baselineskip
\textbf{Observed Behaviour:}
\begin{enumerate}
    \item When asked to register for an account, majority of participants would enter text into the sign in form and not realise they had to press the register button first.
    \item When asked to find their friends list, a few participants did not find the icon representing the friends list screen significant, and, struggled to navigate to this screen.
\end{enumerate}

\textbf{Potential Solutions:}
\begin{enumerate}
    \item A screen could be presented to the user that has clear sign in, and, register buttons before corresponding forms are presented to the user. 
    \item A text label below the friends list icon could be added to help users recognise how they are to access this screen.
\end{enumerate}
\vskip\baselineskip

\subsection{Acceptance Criteria}

The completion of the non-functional requirements listed in \ref{non-functional} can be assessed from these evaluation results. The applications navigation system was praised through user feedback and received positive responses to the navigation related questions in both the in person, and online experiments. Another non-functional requirement listed was that the application should have a visually pleasing user interface throughout. The consistent, and aesthetically pleasing user interface was praised throughout with most participants responding positively to design related questions. No accessibility issues were noted, with considerations for accessibility, and usability made throughout the design, and implementation phases of the project. The application loading times cannot be statistically measured due to loading times dependent on multiple variables such as, differing devices, operating systems, and internet connectivity. However, there were no issues observed during the evaluation, or development concerning the application loading times. A `won't have' requirement listed was the integration of dark mode, this was not completed during this release but could be in the future with the implementation of a dark mode setting. Overall, the prioritised non-functional requirements were met, and users reacted positively to the application, therefore the product can be accepted. 

\section{Summary}

In summary, this chapter discussed the evaluation process of the application. Application testing was performed throughout the project through user, and Jest testing. User testing was performed by fellow students, and the application developer to ensure new features performed correctly. Jest testing was used to ensure the user interface rendered correctly by using snapshot tests, and application behaviour was as expected with unit tests. Two experiments were then conducted, an in-person evaluation where participants would have access to the application, and an online evaluation showcasing screenshots and recordings of an example user utilising the application. This approach was optimal for gathering a suitable number of responses due to the limitation of a single device. Various methods such as single ease questions, free response feedback, and the system usability scale were used to evaluate the application with positive results and feedback gathered. Overall, the evaluation was a success with participants showing a clear interest for the application and providing valuable feedback. 
